# Preparing VEP to run with Hail

## Motivation

Standard Hail Query [hl.vep](https://hail.is/docs/0.2/methods/genetics.html#hail.methods.vep) function has 2 drawbacks:

* It doesn't support Batch backend;
* It's limited to VEP version 95 (under the hood it's hardcoded to use the image `konradjk/vep95_loftee:0.2` which is built in 2019 for VEP 95, with no source Dockerfile available).

Eventually `hl.vep` will fully support custom VEP versions and Batch backend, but as a temporary solution, we have an alternative process described below.

## Preparing reference data

The following steps describe how to prepare reference data for running VEP of any version, both for Hail Batch and Hail Query.

1. Choose the VEP version you want to use. Make sure it's available on [Bioconda](https://anaconda.org/bioconda/ensembl-vep/files). Replace `$VEP_VERSION` below accordingly (e.g. `105.0`).

1. Update the VEP Docker image: Trigger the [`Deploy container` GitHub CI workflow](https://github.com/populationgenomics/images/actions/workflows/deploy.yaml) in the [`images` repository](https://github.com/populationgenomics/images) using `vep` as the `Name of image` and `$VEP_VERSION` as the `Tag of image` parameter.

1. Rebuild the VEP cache and LOFTEE reference data bundles:

   Create a local config for the new version, e.g. in `$HOME/tmp/vep_$VEP_VERSION.toml`, making sure to replace the version literals below:

   ```toml
   [images]
   vep = "australia-southeast1-docker.pkg.dev/cpg-common/images/vep:105.0"

   [references]
   vep_mount = "gs://cpg-common-main/references/vep/105.0/mount"
   ```

   The VEP cache bundle is huge, so we use Hail Batch to copy it from Ensemble FTP servers and the Broad Institute servers to the CPG reference GCP bucket:

    ```bash
    analysis-runner --dataset common --access-level standard --description "Build resources for VEP $VEP_VERSION" --output-dir=vep/$VEP_VERSION --config=$HOME/tmp/vep_$VEP_VERSION.toml python3 copy-references.py $VEP_VERSION
    ```

1. If you want to permanently update the default VEP version for [`production-pipelines`](https://github.com/populationgenomics/production-pipelines) workflows:
   * Update the version for `vep` in [`images.toml`](https://github.com/populationgenomics/images/blob/main/images.toml). This map translates to `cpg_utils.image_path('vep')` used in Hail Batch workflows.
   * Update the version for `vep_mount` in [`references.py`](https://github.com/populationgenomics/references/blob/main/references.py). This map translates to `cpg_utils.reference_path('vep_mount')` used in Hail Batch workflows.

## Running using Hail Batch

Standard Hail Query `hl.vep` does not support Batch Backend, so we use a workaround for that:

* Split input VCF into partitions using an interval list generated by Picard tools; 
* Submit Hail Batch jobs that call VEP with reference data in a mounted volume `vep_mount`, as well as the previously built `vep` image;
* For the VEP command, we parametrise it to write results using JSON format; 
* Finally, we gather the JSON results into a Hail Table using a pre-prepared schema, which is borrowed from Hail Query's `hl.vep`. 

The whole method is implemented in [CPG workflows](https://github.com/populationgenomics/production-pipelines/blob/main/cpg_workflows/jobs/vep.py) as part of the Seqr Loader workflow. See example of usage in [test/test-batch-backend.py](test/test-batch-backend.py).

## Running using `hl.vep`

The Query's `hl.vep` function works only with the Spark backend on a Dataproc cluster. To make it work with a custom VEP version, the following extra steps are required.

1. The dataproc initialisation script in this repository [dataproc-init.sh](dataproc-init.sh) is copied from the [Hail codebase](https://github.com/hail-is/hail/blob/cc0a051740f4de08408e6a2094ffcb1c3158ee9c/hail/python/hailtop/hailctl/dataproc/resources/vep-GRCh38.sh) and adjusted to pull the image from CPG artefact registry, and parameterised by `VEP_VERSION`.

Script is parametrised with `__VEP_VERSION__`, so pass it through `sed` when copying to the bucket:

```shell
cat dataproc-init.sh | sed "s/__VEP_VERSION__/${VEP_VERSION}/g" | gsutil cp - gs://cpg-reference/vep/${VEP_VERSION}/dataproc/init.sh
```

2. Similarly, the JSON config script for dataproc [dataproc-config.json](dataproc-config.json) is also copied from the [Hail codebase](https://github.com/hail-is/hail/blob/cc0a051740f4de08408e6a2094ffcb1c3158ee9c/hail/python/hailtop/hailctl/hdinsight/resources/vep-GRCh38.json) and modified to reflect the newer VEP versions, and additionally has `mane_select:String,mane_plus_clinical:String` in schema to load MANE IDs that are supported by modern versions of VEP.

The config is parametrised with `__VEP_VERSION__`, so pass it through `sed` when copying to the bucket:

```sh
cat dataproc-config.json | sed "s/__VEP_VERSION__/${VEP_VERSION}/g" | gsutil cp - gs://cpg-reference/vep/${VEP_VERSION}/dataproc/config.json
```

We also have a separate version of the config `dataproc-config-no-exac.json` for older VEP versions (before v88). In this version, ExAC fields are excluded: they used to contain non-numerical format for ExAC frequencies, e.g. `"exac_adj_maf":"-:0.2934,-:8.292e-06", "exac_maf":"-:0.293,-:8.261e-06"`, which would break parsing into Hail using the schema where those fields are specified as floats. 

3. After all is set up, you can start a Dataproc cluster passing the initialization script explicitly with `init`, instead of using the `vep` parameter. See example in [test/test-dataproc-wrapper.py](test/test-dataproc-wrapper.py). Make sure you set larger resources for the workers (the highmem machine type and larger storage). Hail would do that automatically with `--vep`, but with a custom `--init` we have to do that explicitly.

Within scripts that you submit to that cluster, you can call the `hl.vep` function with explicitly provided `config` parameter (otherwise it would attempt to get the VEP_CONFIG environment variable, which is only set with `--vep`). See example in [test/test-dataproc-script.py](test/test-dataproc-script.py).
