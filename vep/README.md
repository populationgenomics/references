# Preparing VEP to run with Hail

## Motivation

Standard Hail Query [hl.vep](https://hail.is/docs/0.2/methods/genetics.html#hail.methods.vep) function has 2 drawbacks:

* It doesn't support Batch backend;
* It's limited to VEP version 95 (under the hood it's hardcoded to use the image `konradjk/vep95_loftee:0.2` which is built in 2019 for VEP 95, with no source Dockerfile available).

Eventually `hl.vep` will fully support custom VEP versions and Batch backend, but as a temporary solution, we have an alternative process described below.

## Preparing reference data

The following steps describe how to prepare reference data for running VEP of any version, both for Hail Batch or Hail Query.

1. Choose the VEP version you want to use. Make sure it's available on [Bioconda](https://anaconda.org/bioconda/ensembl-vep/files). Set the selected version to the environment variable `export VEP_VERSION=104.3`.

2. Update the VEP Docker image:
   * Add (or update) the `vep` Dockerfile in the [`images` repository](https://github.com/populationgenomics/images/blob/07a2580c67886412ce1f0293274e7bd5e202a868/images/vep/Dockerfile), and trigger the GitHub CI workflow using $VEP_VERSION as a `tag` parameter.
   * Add (or update) a line `vep = 'vep:104.3'` in the images names map `[images]` in the [workflows config TOML](https://github.com/populationgenomics/production-pipelines/blob/main/configs/defaults/workflows.toml#L93) to reflect this new version. This map translates into `cpg_utils.image_path('vep')` used in Hail Batch workflows.

3. Rebuild the VEP cache and LOFTEE reference data bundles:
   * The VEP cache bundle is huge, so we use Hail Batch to copy it from Ensemble FTP servers and the Broad Institute servers to the CPG reference GCP bucket:

    ```bash
    python copy-references.py $VEP_VERSION
    ```
    
    * Add (or update) a line `vep_mount = 'vep/105.0/mount'` in the reference data map `[references]` in the [workflows config TOML](https://github.com/populationgenomics/production-pipelines/blob/main/configs/defaults/workflows.toml#L111) to reflect this new version. This map translates into `cpg_utils.reference_path('vep_mount')` used in Hail Batch workflows.


## Running using Hail Batch

Standard Hail Query `hl.vep` does not support Batch Backend, so we use a workaround for that:

* Split input VCF into partitions using an interval list generated by Picard tools; 
* Submit Hail Batch jobs that call VEP with reference data in a mounted volume `vep_mount`, as well as the previously built `vep` image;
* For the VEP command, we parametrise it to write results using JSON format; 
* Finally, we gather the JSON results into a Hail Table using a pre-prepared schema, which is borrowed from Hail Query's `hl.vep`. 

The whole method is implemented in [CPG workflows](https://github.com/populationgenomics/production-pipelines/blob/main/cpg_workflows/jobs/vep.py) as part of the Seqr Loader workflow. See example of usage in [test/test-batch-backend.py](test/test-batch-backend.py).

## Running using `hl.vep`

The Query's `hl.vep` function works only with the Spark backend on a Dataproc cluster. To make it work with a custom VEP version, the following extra steps are required.

1. The dataproc initialisation script in this repository [dataproc-init.sh](dataproc-init.sh) is copied from the [Hail codebase](https://github.com/hail-is/hail/blob/cc0a051740f4de08408e6a2094ffcb1c3158ee9c/hail/python/hailtop/hailctl/dataproc/resources/vep-GRCh38.sh) and adjusted to pull the image from CPG artefact registry, and parameterised by `VEP_VERSION`.

Script is parametrised with `__VEP_VERSION__`, so pass it through `sed` when copying to the bucket:

```shell
cat dataproc-init.sh | sed "s/__VEP_VERSION__/${VEP_VERSION}/g" | gsutil cp - gs://cpg-common-main/references/vep/${VEP_VERSION}/dataproc/init.sh
```

2. Similarly, the JSON config script for dataproc [dataproc-config.json](dataproc-config.json) is also copied from the [Hail codebase](https://github.com/hail-is/hail/blob/cc0a051740f4de08408e6a2094ffcb1c3158ee9c/hail/python/hailtop/hailctl/hdinsight/resources/vep-GRCh38.json) and modified to reflect the newer VEP versions, and additionally has `mane_select:String,mane_plus_clinical:String` in schema to load MANE IDs that are supported by modern versions of VEP.

The config is parametrised with `__VEP_VERSION__`, so pass it through `sed` when copying to the bucket:

```sh
cat dataproc-config.json | sed "s/__VEP_VERSION__/${VEP_VERSION}/g" | gsutil cp - gs://cpg-common-main/references/vep/${VEP_VERSION}/dataproc/config.json
```

We also have a separate version of the config `dataproc-config-no-exac.json` for older VEP versions (before v88). In this version, ExAC fields are excluded: they used to contain non-numerical format for ExAC frequencies, e.g. `"exac_adj_maf":"-:0.2934,-:8.292e-06", "exac_maf":"-:0.293,-:8.261e-06"`, which would break parsing into Hail using the schema where those fields are specified as floats. 

3. After all is set up, you can start a Dataproc cluster passing the initialization script explicitly with `init`, instead of using the `vep` parameter. See example in [test/test-dataproc-wrapper.py](test/test-dataproc-wrapper.py). Make sure you set larger resources for the workers (the highmem machine type and larger storage). Hail would do that automatically with `--vep`, but with a custom `--init` we have to do that explicitly.

Within scripts that you submit to that cluster, you can call the `hl.vep` function with explicitly provided `config` parameter (otherwise it would attempt to get the VEP_CONFIG environment variable, which is only set with `--vep`). See example in [test/test-dataproc-script.py](test/test-dataproc-script.py).
